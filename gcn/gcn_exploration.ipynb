{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN Code Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undestanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Read index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"N-hot encoding. idx conatains multiple indices, \n",
    "    so the output contains multiple elements that are set to True\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"dataset_str: {}\".format(dataset_str))\n",
    "    \n",
    "    # read all the pickled files\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        data_file = \"data/ind.{}.{}\".format(dataset_str, names[i])\n",
    "        print(\"reading: \", data_file)\n",
    "        with open(data_file, 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    \n",
    "    print(\"\\nGraph:\")\n",
    "    for i in range(10):\n",
    "        print(\"Node {}: {}\".format(i, graph[i]))\n",
    "        \n",
    "    # Read index file\n",
    "    index_file = \"data/ind.{}.test.index\".format(dataset_str)\n",
    "    test_idx_reorder = parse_index_file(index_file)\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    # stack up 2 csr_matrix objects and Convert this matrix to List of Lists format.\n",
    "    features = sp.vstack((allx, tx)).tolil() \n",
    "    print(\"\\nFeature Vectors:\")\n",
    "    print(\"Number of Features: {}\".format(x.shape[1]))\n",
    "    print(\"Number of Training Nodes: {}\".format(x.shape[0]))\n",
    "    print(\"allx.shape: {} - feature vectors of all training nodes, including labeled and unlabeled (superset of x)\".format(allx.shape))\n",
    "    print(\"tx.shape:   {} - feature vectors of the test nodes\".format(tx.shape))\n",
    "    print(\"features:   {} = allx{}  +   tx{}\".format(features.shape, allx.shape, tx.shape))\n",
    "    \n",
    "    # reorder the feature vectors to match the node order used in the graph matrix\n",
    "    # assign the range of rows from the features matrix that contain all the test \n",
    "    # feature vectors to their correct positions in the feature matrix\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    \n",
    "    # create the adjacency matrix from the graph object\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    print(\"\\nLabels:\")\n",
    "    print(\"Number of Classes: {}\".format(y.shape[1]))\n",
    "    print(\"Number of Training Nodes: {}\".format(y.shape[0]))\n",
    "    print(\"Number of Training Nodes per Class: {}\".format(y.shape[0]//y.shape[1]))\n",
    "    print(\"y.shape:       {} - one-hot labels of labeled training nodes\".format(y.shape))\n",
    "    print(\"ally.shape:   {} - labels of training nodes, including labeled and unlabeled (superset of y)\".format(ally.shape))\n",
    "    print(\"ty.shape:     {} - labels of the test nodes\".format(ty.shape))\n",
    "    print(\"labels:       {} = ally{} + ty{}\".format(labels.shape, ally.shape, ty.shape))\n",
    "\n",
    "    # reorder the labels to match the node order used in the graph matrix ??\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    print(\"\\nfile: \", index_file)\n",
    "    print(\"Reorder test feature vectors: \")\n",
    "    print(\"   test_idx_reorder:  {} ... {}\".format(test_idx_reorder[:5], test_idx_reorder[-5:]))\n",
    "    print(\"   len(test_idx_reorder): {} - range: [{} .. {}] - indices of test nodes in the graph\".format(len(test_idx_reorder), min(test_idx_reorder), max(test_idx_reorder)))\n",
    "    print(\" - The indices map from the sequential order in which vectors were stored in the in tx and ty files\")\n",
    "    print(\"   to a position according to the ID (index) used to identify them in the graph dictionary.\")\n",
    "    print(\"   This is a reshuffle of the test vectors within their own row range in the 'features' matrix [{} .. {}]\".format(min(test_idx_reorder), max(test_idx_reorder)))\n",
    "    print(\"\\n   test_idx_range:  {} ... {}\".format(test_idx_range[:5], test_idx_range[-5:]))\n",
    "    print(\"   len(test_idx_range): {} - range: [{} .. {}] - indices of test nodes in the graph\".format(len(test_idx_range), min(test_idx_range), max(test_idx_range)))\n",
    "    \n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "    print(\"\\nSplitting the dataset into train, val and test subsets:\")\n",
    "    print(\"idx_train: range=[{}, {}] len={}\".format(min(idx_train), max(idx_train), len(idx_train)))\n",
    "    print(\"idx_val:   range=[{}, {}] len={}\".format(min(idx_val), max(idx_val), len(idx_val)))\n",
    "    print(\"idx_test:  range=[{}, {}] len={}\".format(min(idx_test), max(idx_test), len(idx_test)))\n",
    "\n",
    "    # Create mask arrays (N-hot encoding)\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "    print(\"\\nCreate one boolean mask array (N-hot encoding) for each subset:\")\n",
    "    print(\"train_mask: shape={}   number of True={}   \\tnumber of False={}\".format(train_mask.shape, train_mask.sum(), sum([1 if not x else 0 for x in train_mask])))\n",
    "    print(\"val_mask:   shape={}   number of True={}   \\tnumber of False={}\".format(val_mask.shape, val_mask.sum(), sum([1 if not x else 0 for x in val_mask])))\n",
    "    print(\"test_mask:  shape={}   number of True={}   \\tnumber of False={}\".format(test_mask.shape, test_mask.sum(), sum([1 if not x else 0 for x in test_mask])))\n",
    "\n",
    "    # create an zero-value labels matrix for each subset\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    print(\"\\ncreate an zero-value labels matrix for each subset\")\n",
    "    \n",
    "    # copy the labels of each subset into each subsets' label matrix\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "    print(\"copy the labels of each subset into each subsets' label matrix:\")\n",
    "    print(\"y_train: shape={}\".format(y_train.shape))\n",
    "    print(\"y_val:   shape={}\".format(y_val.shape))\n",
    "    print(\"y_test:  shape={}\".format(y_test.shape))\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "options = {}\n",
    "options['dataset'] = 'pubmed' # 'cora'] = 'citeseer'] = 'pubmed'\n",
    "options['model'] = 'gcn' # 'gcn'] = 'gcn_cheby'] = 'dense'\n",
    "options['learning_rate'] = 0.01\n",
    "options['epochs'] = 200\n",
    "options['hidden1'] = 16\n",
    "options['dropout'] = 0.5\n",
    "options['weight_decay'] = 5e-4\n",
    "options['early_stopping'] = 10\n",
    "options['max_degree'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_str: pubmed\n",
      "reading:  data/ind.pubmed.x\n",
      "reading:  data/ind.pubmed.y\n",
      "reading:  data/ind.pubmed.tx\n",
      "reading:  data/ind.pubmed.ty\n",
      "reading:  data/ind.pubmed.allx\n",
      "reading:  data/ind.pubmed.ally\n",
      "reading:  data/ind.pubmed.graph\n",
      "\n",
      "Graph:\n",
      "Node 0: [14442, 1378, 1544, 6092, 7636]\n",
      "Node 1: [10199, 8359, 2943]\n",
      "Node 2: [11485, 15572, 10471]\n",
      "Node 3: [8249]\n",
      "Node 4: [14044]\n",
      "Node 5: [1312, 12968]\n",
      "Node 6: [17284, 8661, 3150, 18614, 7296, 2216, 8981, 13656, 6572, 3509, 13655, 12098, 7691, 9232, 7335, 4464, 2128, 16720, 767, 6697, 18121, 10265]\n",
      "Node 7: [15577, 14688, 17955, 18425, 6242, 2343, 17354, 14754, 5564, 2019, 19376, 1568, 14843, 1588, 4058, 10243, 8335]\n",
      "Node 8: [3157]\n",
      "Node 9: [11186, 7875, 221, 12664, 1456, 7956, 8915, 19447, 16622]\n",
      "\n",
      "Feature Vectors:\n",
      "Number of Features: 500\n",
      "Number of Training Nodes: 60\n",
      "allx.shape: (18717, 500) - feature vectors of all training nodes, including labeled and unlabeled (superset of x)\n",
      "tx.shape:   (1000, 500) - feature vectors of the test nodes\n",
      "features:   (19717, 500) = allx(18717, 500)  +   tx(1000, 500)\n",
      "\n",
      "Labels:\n",
      "Number of Classes: 3\n",
      "Number of Training Nodes: 60\n",
      "Number of Training Nodes per Class: 20\n",
      "y.shape:       (60, 3) - one-hot labels of labeled training nodes\n",
      "ally.shape:   (18717, 3) - labels of training nodes, including labeled and unlabeled (superset of y)\n",
      "ty.shape:     (1000, 3) - labels of the test nodes\n",
      "labels:       (19717, 3) = ally(18717, 3) + ty(1000, 3)\n",
      "\n",
      "file:  data/ind.pubmed.test.index\n",
      "Reorder test feature vectors: \n",
      "   test_idx_reorder:  [18747, 19392, 19181, 18843, 19221] ... [19500, 19307, 19288, 19594, 19271]\n",
      "   len(test_idx_reorder): 1000 - range: [18717 .. 19716] - indices of test nodes in the graph\n",
      " - The indices map from the sequential order in which vectors were stored in the in tx and ty files\n",
      "   to a position according to the ID (index) used to identify them in the graph dictionary.\n",
      "   This is a reshuffle of the test vectors within their own row range in the 'features' matrix [18717 .. 19716]\n",
      "\n",
      "   test_idx_range:  [18717 18718 18719 18720 18721] ... [19712 19713 19714 19715 19716]\n",
      "   len(test_idx_range): 1000 - range: [18717 .. 19716] - indices of test nodes in the graph\n",
      "\n",
      "Splitting the dataset into train, val and test subsets:\n",
      "idx_train: range=[0, 59] len=60\n",
      "idx_val:   range=[60, 559] len=500\n",
      "idx_test:  range=[18717, 19716] len=1000\n",
      "\n",
      "Create one boolean mask array (N-hot encoding) for each subset:\n",
      "train_mask: shape=(19717,)   number of True=60   \tnumber of False=19657\n",
      "val_mask:   shape=(19717,)   number of True=500   \tnumber of False=19217\n",
      "test_mask:  shape=(19717,)   number of True=1000   \tnumber of False=18717\n",
      "\n",
      "create an zero-value labels matrix for each subset\n",
      "copy the labels of each subset into each subsets' label matrix:\n",
      "y_train: shape=(19717, 3)\n",
      "y_val:   shape=(19717, 3)\n",
      "y_test:  shape=(19717, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# from utils import load_data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(options['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph has 19717 nodes and 88651 edges.\n",
      "\n",
      "(19717, 19717)\n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The graph has {} nodes and {} edges.\\n\\n{}\\n\".format(adj.shape[0], adj.todense().sum(), adj.shape))\n",
    "print(adj.todense()[0:20,0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19717 nodes and 3 classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} nodes and {} classes.\".format(y_train.shape[0], y_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19717 19717 19717\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), len(y_val), len(y_test))\n",
    "print(y_train[:3])\n",
    "print(y_val[:3])\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Math\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency Matrix Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    print(\"adj=\\n\", adj.todense())\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    print('adj.sum(1)=\\n', rowsum)\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    print('d_inv_sqrt=\\n', d_inv_sqrt)\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    print('d_mat_inv_sqrt=\\n', d_mat_inv_sqrt.todense())\n",
    "    print('adj.dot(d_mat_inv_sqrt)=\\n', adj.dot(d_mat_inv_sqrt).todense())\n",
    "    print('adj.dot(d_mat_inv_sqrt).transpose()=\\n', adj.dot(d_mat_inv_sqrt).transpose().todense())\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 3])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = np.array([[0, 1], [1, 3]])\n",
    "adj.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj=\n",
      " [[0 1]\n",
      " [1 3]]\n",
      "adj.sum(1)=\n",
      " [[1]\n",
      " [4]]\n",
      "d_inv_sqrt=\n",
      " [1.  0.5]\n",
      "d_mat_inv_sqrt=\n",
      " [[1.  0. ]\n",
      " [0.  0.5]]\n",
      "adj.dot(d_mat_inv_sqrt)=\n",
      " [[0.  0.5]\n",
      " [1.  1.5]]\n",
      "adj.dot(d_mat_inv_sqrt).transpose()=\n",
      " [[0.  1. ]\n",
      " [0.5 1.5]]\n"
     ]
    }
   ],
   "source": [
    "normalized_adj = normalize_adj(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_adj=\n",
      " [[0.   0.5 ]\n",
      " [0.5  0.75]]\n"
     ]
    }
   ],
   "source": [
    "print('normalized_adj=\\n', normalized_adj.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing data to Normalized Files\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "def write_data(dataset, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, graph):\n",
    "    \"\"\"\n",
    "    Write input data to gcn/data_out directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instans\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    # write all the pickled files\n",
    "    objects_dic = {\n",
    "        'x': x, \n",
    "        'y': y, \n",
    "        'tx': tx, \n",
    "        'ty': ty, \n",
    "        'allx': allx, \n",
    "        'ally': ally, \n",
    "        'graph': graph\n",
    "    }\n",
    "    \n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        data_file = \"data_out/ind.{}.{}\".format(dataset, names[i])\n",
    "        print(\"writing: \", data_file)\n",
    "        with open(data_file, 'wb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.dump(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.dump(f))\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"pubmed\"\n",
    "write_data(dataset, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "* * *\n",
    "\n",
    "1. Convert SROIE labels into a graph (Lohani et al., 2019, An Invoice Reading System Using a Graph Convolutional Network.)\n",
    "    - Categories: \n",
    "        - company\n",
    "        * date\n",
    "        * address\n",
    "        * total\n",
    "* Write SROIE Graph as normalized dataset expeced by Kipf's source code. Call the write_data() function that we developed in this notebook.\n",
    "* Train GCN and call it on the unlabeled documents.\n",
    "* Measure accuracy\n",
    "* Write results, make corrections, feedback, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SROE dataset\n",
    "dataset_str = \"0325updated.task1train(626p)\"\n",
    "task1train_dir = \"/home/adrian/as/blogs/nanonets/datasets/SROIE2019-20191212T043930Z-001/SROIE2019/{}/\".format(dataset_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore\n",
    "* * * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "def ducuments_to_csr_matrix(docs, debug=False):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for d in docs:\n",
    "        for term in d:\n",
    "\n",
    "            # vocabulary: each word is inserted as key in the dictionary (if it doesnt exist)\n",
    "            # and the value is the index which is given by the length of the dictionary at the moment of insertion\n",
    "            index = vocabulary.setdefault(term, len(vocabulary))\n",
    "\n",
    "            # indices: a flat list of indices to each word of each document, in document input order.\n",
    "            indices.append(index)\n",
    "\n",
    "            # data: a list of 1's\n",
    "            data.append(1)\n",
    "\n",
    "        # indptr: index to index. It contains one pointer per document, \n",
    "        # is a pointer to the pointer to the first word of a document in the indices array.\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    x = csr_matrix((data, indices, indptr), dtype=int).toarray()\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Details to understand the csr_matrix structure.\")\n",
    "        print(\"\\nvocabulary={}\\n\".format(vocabulary))\n",
    "        print(\"docs={}\".format(docs))\n",
    "        print('\\nindices: a flat list of indices to each word of each document, in document input order.')\n",
    "        print(\"indices={}\".format(indices))\n",
    "        print(\"\\nindptr: index to index. Contains pointers to the positions in the indices array that point to the first word of each document.\")\n",
    "        print(\"indptr={}\".format(indptr))\n",
    "        print(\"csr_matrix=\\n{}\".format(x))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details to understand the csr_matrix structure.\n",
      "\n",
      "vocabulary={'hello': 0, 'world': 1, 'goodbye': 2, 'cruel': 3}\n",
      "\n",
      "docs=[['hello', 'world', 'hello'], ['goodbye', 'cruel', 'world']]\n",
      "\n",
      "indices: a flat list of indices to each word of each document, in document input order.\n",
      "indices=[0, 1, 0, 2, 3, 1]\n",
      "\n",
      "indptr: index to index. Contains pointers to the positions in the indices array that point to the first word of each document.\n",
      "indptr=[0, 3, 6]\n",
      "csr_matrix=\n",
      "[[2 1 0 0]\n",
      " [0 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0],\n",
       "       [0, 1, 1, 1]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to understand the csr_matrix structure.\n",
    "docs = [[\"hello\", \"world\", \"hello\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
    "x = ducuments_to_csr_matrix(docs, True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
