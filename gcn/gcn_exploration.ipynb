{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN Code Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undestanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Read index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"N-hot encoding. idx conatains multiple indices, \n",
    "    so the output contains multiple elements that are set to True\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"dataset_str: {}\".format(dataset_str))\n",
    "    \n",
    "    # read all the pickled files\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        data_file = \"data/ind.{}.{}\".format(dataset_str, names[i])\n",
    "        print(\"reading: \", data_file)\n",
    "        with open(data_file, 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    \n",
    "    print(\"\\nGraph:\")\n",
    "    for i in range(10):\n",
    "        print(\"Node {}: {}\".format(i, graph[i]))\n",
    "        \n",
    "    # Read index file\n",
    "    index_file = \"data/ind.{}.test.index\".format(dataset_str)\n",
    "    test_idx_reorder = parse_index_file(index_file)\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    # stack up 2 csr_matrix objects and Convert this matrix to List of Lists format.\n",
    "    features = sp.vstack((allx, tx)).tolil() \n",
    "    print(\"\\nFeature Vectors:\")\n",
    "    print(\"Number of Features: {}\".format(x.shape[1]))\n",
    "    print(\"Number of Training Nodes: {}\".format(x.shape[0]))\n",
    "    print(\"allx.shape: {} - feature vectors of all training nodes, including labeled and unlabeled (superset of x)\".format(allx.shape))\n",
    "    print(\"tx.shape:   {} - feature vectors of the test nodes\".format(tx.shape))\n",
    "    print(\"features:   {} = allx{}  +   tx{}\".format(features.shape, allx.shape, tx.shape))\n",
    "    \n",
    "    # reorder the feature vectors to match the node order used in the graph matrix\n",
    "    # assign the range of rows from the features matrix that contain all the test \n",
    "    # feature vectors to their correct positions in the feature matrix\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    \n",
    "    # create the adjacency matrix from the graph object\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    print(\"\\nLabels:\")\n",
    "    print(\"Number of Classes: {}\".format(y.shape[1]))\n",
    "    print(\"Number of Training Nodes: {}\".format(y.shape[0]))\n",
    "    print(\"Number of Training Nodes per Class: {}\".format(y.shape[0]//y.shape[1]))\n",
    "    print(\"y.shape:       {} - one-hot labels of labeled training nodes\".format(y.shape))\n",
    "    print(\"ally.shape:   {} - labels of training nodes, including labeled and unlabeled (superset of y)\".format(ally.shape))\n",
    "    print(\"ty.shape:     {} - labels of the test nodes\".format(ty.shape))\n",
    "    print(\"labels:       {} = ally{} + ty{}\".format(labels.shape, ally.shape, ty.shape))\n",
    "\n",
    "    # reorder the labels to match the node order used in the graph matrix ??\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    print(\"\\nfile: \", index_file)\n",
    "    print(\"Reorder test feature vectors: \")\n",
    "    print(\"   test_idx_reorder:  {} ... {}\".format(test_idx_reorder[:5], test_idx_reorder[-5:]))\n",
    "    print(\"   len(test_idx_reorder): {} - range: [{} .. {}] - indices of test nodes in the graph\".format(len(test_idx_reorder), min(test_idx_reorder), max(test_idx_reorder)))\n",
    "    print(\" - The indices map from the sequential order in which vectors were stored in the in tx and ty files\")\n",
    "    print(\"   to a position according to the ID (index) used to identify them in the graph dictionary.\")\n",
    "    print(\"   This is a reshuffle of the test vectors within their own row range in the 'features' matrix [{} .. {}]\".format(min(test_idx_reorder), max(test_idx_reorder)))\n",
    "    print(\"\\n   test_idx_range:  {} ... {}\".format(test_idx_range[:5], test_idx_range[-5:]))\n",
    "    print(\"   len(test_idx_range): {} - range: [{} .. {}] - indices of test nodes in the graph\".format(len(test_idx_range), min(test_idx_range), max(test_idx_range)))\n",
    "    \n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "    print(\"\\nSplitting the dataset into train, val and test subsets:\")\n",
    "    print(\"idx_train: range=[{}, {}] len={}\".format(min(idx_train), max(idx_train), len(idx_train)))\n",
    "    print(\"idx_val:   range=[{}, {}] len={}\".format(min(idx_val), max(idx_val), len(idx_val)))\n",
    "    print(\"idx_test:  range=[{}, {}] len={}\".format(min(idx_test), max(idx_test), len(idx_test)))\n",
    "\n",
    "    # Create mask arrays (N-hot encoding)\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "    print(\"\\nCreate one boolean mask array (N-hot encoding) for each subset:\")\n",
    "    print(\"train_mask: shape={}   number of True={}   \\tnumber of False={}\".format(train_mask.shape, train_mask.sum(), sum([1 if not x else 0 for x in train_mask])))\n",
    "    print(\"val_mask:   shape={}   number of True={}   \\tnumber of False={}\".format(val_mask.shape, val_mask.sum(), sum([1 if not x else 0 for x in val_mask])))\n",
    "    print(\"test_mask:  shape={}   number of True={}   \\tnumber of False={}\".format(test_mask.shape, test_mask.sum(), sum([1 if not x else 0 for x in test_mask])))\n",
    "\n",
    "    # create an zero-value labels matrix for each subset\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    print(\"\\ncreate an zero-value labels matrix for each subset\")\n",
    "    \n",
    "    # copy the labels of each subset into each subsets' label matrix\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "    print(\"copy the labels of each subset into each subsets' label matrix:\")\n",
    "    print(\"y_train: shape={}\".format(y_train.shape))\n",
    "    print(\"y_val:   shape={}\".format(y_val.shape))\n",
    "    print(\"y_test:  shape={}\".format(y_test.shape))\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "options = {}\n",
    "options['dataset'] = 'pubmed' # 'cora'] = 'citeseer'] = 'pubmed'\n",
    "options['model'] = 'gcn' # 'gcn'] = 'gcn_cheby'] = 'dense'\n",
    "options['learning_rate'] = 0.01\n",
    "options['epochs'] = 200\n",
    "options['hidden1'] = 16\n",
    "options['dropout'] = 0.5\n",
    "options['weight_decay'] = 5e-4\n",
    "options['early_stopping'] = 10\n",
    "options['max_degree'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_str: pubmed\n",
      "reading:  data/ind.pubmed.x\n",
      "reading:  data/ind.pubmed.y\n",
      "reading:  data/ind.pubmed.tx\n",
      "reading:  data/ind.pubmed.ty\n",
      "reading:  data/ind.pubmed.allx\n",
      "reading:  data/ind.pubmed.ally\n",
      "reading:  data/ind.pubmed.graph\n",
      "\n",
      "Graph:\n",
      "Node 0: [14442, 1378, 1544, 6092, 7636]\n",
      "Node 1: [10199, 8359, 2943]\n",
      "Node 2: [11485, 15572, 10471]\n",
      "Node 3: [8249]\n",
      "Node 4: [14044]\n",
      "Node 5: [1312, 12968]\n",
      "Node 6: [17284, 8661, 3150, 18614, 7296, 2216, 8981, 13656, 6572, 3509, 13655, 12098, 7691, 9232, 7335, 4464, 2128, 16720, 767, 6697, 18121, 10265]\n",
      "Node 7: [15577, 14688, 17955, 18425, 6242, 2343, 17354, 14754, 5564, 2019, 19376, 1568, 14843, 1588, 4058, 10243, 8335]\n",
      "Node 8: [3157]\n",
      "Node 9: [11186, 7875, 221, 12664, 1456, 7956, 8915, 19447, 16622]\n",
      "\n",
      "Feature Vectors:\n",
      "Number of Features: 500\n",
      "Number of Training Nodes: 60\n",
      "allx.shape: (18717, 500) - feature vectors of all training nodes, including labeled and unlabeled (superset of x)\n",
      "tx.shape:   (1000, 500) - feature vectors of the test nodes\n",
      "features:   (19717, 500) = allx(18717, 500)  +   tx(1000, 500)\n",
      "\n",
      "Labels:\n",
      "Number of Classes: 3\n",
      "Number of Training Nodes: 60\n",
      "Number of Training Nodes per Class: 20\n",
      "y.shape:       (60, 3) - one-hot labels of labeled training nodes\n",
      "ally.shape:   (18717, 3) - labels of training nodes, including labeled and unlabeled (superset of y)\n",
      "ty.shape:     (1000, 3) - labels of the test nodes\n",
      "labels:       (19717, 3) = ally(18717, 3) + ty(1000, 3)\n",
      "\n",
      "file:  data/ind.pubmed.test.index\n",
      "Reorder test feature vectors: \n",
      "   test_idx_reorder:  [18747, 19392, 19181, 18843, 19221] ... [19500, 19307, 19288, 19594, 19271]\n",
      "   len(test_idx_reorder): 1000 - range: [18717 .. 19716] - indices of test nodes in the graph\n",
      " - The indices map from the sequential order in which vectors were stored in the in tx and ty files\n",
      "   to a position according to the ID (index) used to identify them in the graph dictionary.\n",
      "   This is a reshuffle of the test vectors within their own row range in the 'features' matrix [18717 .. 19716]\n",
      "\n",
      "   test_idx_range:  [18717 18718 18719 18720 18721] ... [19712 19713 19714 19715 19716]\n",
      "   len(test_idx_range): 1000 - range: [18717 .. 19716] - indices of test nodes in the graph\n",
      "\n",
      "Splitting the dataset into train, val and test subsets:\n",
      "idx_train: range=[0, 59] len=60\n",
      "idx_val:   range=[60, 559] len=500\n",
      "idx_test:  range=[18717, 19716] len=1000\n",
      "\n",
      "Create one boolean mask array (N-hot encoding) for each subset:\n",
      "train_mask: shape=(19717,)   number of True=60   \tnumber of False=19657\n",
      "val_mask:   shape=(19717,)   number of True=500   \tnumber of False=19217\n",
      "test_mask:  shape=(19717,)   number of True=1000   \tnumber of False=18717\n",
      "\n",
      "create an zero-value labels matrix for each subset\n",
      "copy the labels of each subset into each subsets' label matrix:\n",
      "y_train: shape=(19717, 3)\n",
      "y_val:   shape=(19717, 3)\n",
      "y_test:  shape=(19717, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "# from utils import load_data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(options['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The graph has 19717 nodes and 88651 edges.\n",
      "\n",
      "(19717, 19717)\n",
      "\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"The graph has {} nodes and {} edges.\\n\\n{}\\n\".format(adj.shape[0], adj.todense().sum(), adj.shape))\n",
    "print(adj.todense()[0:20,0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19717 nodes and 3 classes.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {} nodes and {} classes.\".format(y_train.shape[0], y_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19717 19717 19717\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train), len(y_val), len(y_test))\n",
    "print(y_train[:3])\n",
    "print(y_val[:3])\n",
    "print(y_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Math\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjacency Matrix Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    print(\"adj=\\n\", adj.todense())\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    print('adj.sum(1)=\\n', rowsum)\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    print('d_inv_sqrt=\\n', d_inv_sqrt)\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    print('d_mat_inv_sqrt=\\n', d_mat_inv_sqrt.todense())\n",
    "    print('adj.dot(d_mat_inv_sqrt)=\\n', adj.dot(d_mat_inv_sqrt).todense())\n",
    "    print('adj.dot(d_mat_inv_sqrt).transpose()=\\n', adj.dot(d_mat_inv_sqrt).transpose().todense())\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 3])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj = np.array([[0, 1], [1, 3]])\n",
    "adj.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adj=\n",
      " [[0 1]\n",
      " [1 3]]\n",
      "adj.sum(1)=\n",
      " [[1]\n",
      " [4]]\n",
      "d_inv_sqrt=\n",
      " [1.  0.5]\n",
      "d_mat_inv_sqrt=\n",
      " [[1.  0. ]\n",
      " [0.  0.5]]\n",
      "adj.dot(d_mat_inv_sqrt)=\n",
      " [[0.  0.5]\n",
      " [1.  1.5]]\n",
      "adj.dot(d_mat_inv_sqrt).transpose()=\n",
      " [[0.  1. ]\n",
      " [0.5 1.5]]\n"
     ]
    }
   ],
   "source": [
    "normalized_adj = normalize_adj(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_adj=\n",
      " [[0.   0.5 ]\n",
      " [0.5  0.75]]\n"
     ]
    }
   ],
   "source": [
    "print('normalized_adj=\\n', normalized_adj.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing data to Normalized Files\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "\n",
    "def write_data(dataset, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, graph):\n",
    "    \"\"\"\n",
    "    Write input data to gcn/data_out directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instans\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    \n",
    "    # write all the pickled files\n",
    "    objects_dic = {\n",
    "        'x': x, \n",
    "        'y': y, \n",
    "        'tx': tx, \n",
    "        'ty': ty, \n",
    "        'allx': allx, \n",
    "        'ally': ally, \n",
    "        'graph': graph\n",
    "    }\n",
    "    \n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        data_file = \"data_out/ind.{}.{}\".format(dataset, names[i])\n",
    "        print(\"writing: \", data_file)\n",
    "        with open(data_file, 'wb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.dump(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.dump(f))\n",
    "\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"pubmed\"\n",
    "write_data(dataset, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan\n",
    "* * *\n",
    "\n",
    "1. Develop a write_data() function to save the graph data into the file format expected by Kipf source code.\n",
    "* Convert SROIE labels into a graph, following the method described in Lohani's paper (Lohani et al., 2019, An Invoice Reading System Using a Graph Convolutional Network.)\n",
    "    - SROIE Features: words inside the bounding box\n",
    "    - SROIE Categories: \n",
    "        - company_value\n",
    "        * date_title\n",
    "        * date_value\n",
    "        * address_title\n",
    "        * address_value\n",
    "        * total_title\n",
    "        * total_value\n",
    "* Write SROIE graph in normalized format as expected by Kipf's source code. Call the write_data() function that we developed in this notebook.\n",
    "* Run Kipf's source code on SROIE data. GCN code trains the model, runs inference on the test nodes and measure accuracy\n",
    "* Write results to google doc, make corrections, feedback, etc.\n",
    "    - Problems with Kipf method:\n",
    "        - the reference networks are 1 single big graph, Receipts are one separate graph each.\n",
    "        * In a reference network we can afford to have 20 labeled examples per class, in a receipt there is only one labeled field of each class\n",
    "        * linked nodes in a reference network are alike (papers have the same topics). The fields in a recipt that are close by are not necesarily similar.\n",
    "        * one notable weakness of Spectral Convolution methods is that they are only useful for learning patterns on fixed graphs, and can’t effectively learn on or take into account cases where each observed graph has a different connection structure. This is because each set of calculated eigenvalues is only coherent to apply to the specific graph it was calculated on.\n",
    "    * Alternative Approach: Message-Passing Neural Networks (MPNN)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SROE dataset\n",
    "dataset_str = \"0325updated.task1train(626p)\"\n",
    "task1train_dir = \"/home/adrian/as/blogs/nanonets/datasets/SROIE2019-20191212T043930Z-001/SROIE2019/{}/\".format(dataset_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore\n",
    "* * * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "def ducuments_to_csr_matrix(docs, debug=False):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    vocabulary = {}\n",
    "    for d in docs:\n",
    "        for term in d:\n",
    "\n",
    "            # vocabulary: each word is inserted as key in the dictionary (if it doesnt exist)\n",
    "            # and the value is the index which is given by the length of the dictionary at the moment of insertion\n",
    "            index = vocabulary.setdefault(term, len(vocabulary))\n",
    "\n",
    "            # indices: a flat list of indices to each word of each document, in document input order.\n",
    "            indices.append(index)\n",
    "\n",
    "            # data: a list of 1's\n",
    "            data.append(1)\n",
    "\n",
    "        # indptr: index to index. It contains one pointer per document, \n",
    "        # is a pointer to the pointer to the first word of a document in the indices array.\n",
    "        indptr.append(len(indices))\n",
    "\n",
    "    x = csr_matrix((data, indices, indptr), dtype=int).toarray()\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Details to understand the csr_matrix structure.\")\n",
    "        print(\"\\nvocabulary={}\\n\".format(vocabulary))\n",
    "        print(\"docs={}\".format(docs))\n",
    "        print('\\nindices: a flat list of indices to each word of each document, in document input order.')\n",
    "        print(\"indices={}\".format(indices))\n",
    "        print(\"\\nindptr: index to index. Contains pointers to the positions in the indices array that point to the first word of each document.\")\n",
    "        print(\"indptr={}\".format(indptr))\n",
    "        print(\"csr_matrix=\\n{}\".format(x))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details to understand the csr_matrix structure.\n",
      "\n",
      "vocabulary={'hello': 0, 'world': 1, 'goodbye': 2, 'cruel': 3}\n",
      "\n",
      "docs=[['hello', 'world', 'hello'], ['goodbye', 'cruel', 'world']]\n",
      "\n",
      "indices: a flat list of indices to each word of each document, in document input order.\n",
      "indices=[0, 1, 0, 2, 3, 1]\n",
      "\n",
      "indptr: index to index. Contains pointers to the positions in the indices array that point to the first word of each document.\n",
      "indptr=[0, 3, 6]\n",
      "csr_matrix=\n",
      "[[2 1 0 0]\n",
      " [0 1 1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 0],\n",
       "       [0, 1, 1, 1]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to understand the csr_matrix structure.\n",
    "docs = [[\"hello\", \"world\", \"hello\"], [\"goodbye\", \"cruel\", \"world\"]]\n",
    "x = ducuments_to_csr_matrix(docs, True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformating SROIE labels files as GCN input files\n",
    "* * *\n",
    "as scipy.sparse.csr.csr_matrix:\n",
    "- ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "* ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "* ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances;\n",
    "\n",
    "as numpy.ndarray:\n",
    "- ind.dataset_str.y => the one-hot labels of the labeled training instances;\n",
    "* ind.dataset_str.ty => the one-hot labels of the test instances;\n",
    "* ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx;\n",
    "\n",
    "\n",
    "as collections.defaultdict:\n",
    "- ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]};\n",
    "\n",
    "\n",
    "as list:\n",
    "- ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting.\n",
    "\n",
    "\n",
    "All objects above must be saved using python pickle module.\n",
    "\n",
    "#### Definitions:\n",
    "- param dataset_str: Dataset name\n",
    "* training instance:\n",
    "* feature vector:\n",
    "* labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁med', 'ford', 'shire', '▁b', 'im', 'od', 'al', '▁distribution']\n",
      "this is anarchism\n",
      "101 000\n",
      "[32, 863, 679]\n",
      "anarchism\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 100)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install bpemb\n",
    "from bpemb import BPEmb\n",
    "\n",
    "# load English BPEmb model with default vocabulary size (10k) and 50-dimensional embeddings\n",
    "bpemb_en = BPEmb(lang=\"en\", dim=100)\n",
    "\n",
    "# apply English BPE subword segmentation model\n",
    "print(bpemb_en.encode(\"medfordshire bimodal distribution\"))\n",
    "\n",
    "# Decode byte-pair-encoded text: (reverse segmentation)\n",
    "print(bpemb_en.decode(['▁this', '▁is', '▁an', 'arch', 'ism']))\n",
    "\n",
    "# The encode-decode roundtrip is lossy:\n",
    "print(\"101\", bpemb_en.decode(bpemb_en.encode(\"101\")))\n",
    "\n",
    "# Byte-pair encode text into IDs for performing an embedding lookup:\n",
    "ids = bpemb_en.encode_ids(\"anarchism\")\n",
    "print(ids)\n",
    "print(bpemb_en.decode_ids(ids))\n",
    "    \n",
    "# load Chinese BPEmb model with vocabulary size 100k and default (100-dim) embeddings\n",
    "bpemb_zh = BPEmb(lang=\"zh\", vs=100000)\n",
    "# apply Chinese BPE subword segmentation model\n",
    "bpemb_zh.encode(\"这是一个中文句子\")  # \"This is a Chinese sentence.\"\n",
    "\n",
    "\n",
    "# Byte-pair encode and embed text:\n",
    "bpemb_en.embed(\"anarchism\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ▁b [ 3.57920e-02 -1.91620e-01 -3.95140e-02  1.96240e-01  1.98990e-02\n",
      " -2.07640e-02 -1.94786e-01  1.29007e-01  1.29045e-01 -8.19000e-04\n",
      " -2.93072e-01  1.49086e-01  4.33130e-02  1.55525e-01  2.18574e-01\n",
      " -1.72513e-01  3.01457e-01  1.73882e-01 -3.16232e-01  1.63880e-01\n",
      " -8.40740e-02  4.31424e-01 -1.23822e-01 -3.04110e-01  3.77727e-01\n",
      " -3.65180e-02 -9.24350e-02 -1.11603e-01  7.34410e-02  2.14299e-01\n",
      " -2.24924e-01  1.16902e-01 -4.04236e-01  1.18613e-01 -1.08001e-01\n",
      "  4.63670e-02  1.22694e-01 -1.04957e-01 -1.18560e-01  1.98746e-01\n",
      "  7.80520e-02 -4.80398e-01  2.87600e-02  2.78143e-01 -1.37563e-01\n",
      "  1.94820e-02 -7.68560e-02  1.78576e-01 -6.17260e-02 -6.47780e-02\n",
      " -6.23970e-02  4.24980e-02  6.49300e-02  4.02810e-02  8.61500e-03\n",
      " -2.30266e-01 -1.78978e-01 -1.25859e-01 -8.23780e-02 -1.12904e-01\n",
      " -2.26400e-03 -1.50030e-02 -8.56650e-02 -7.23310e-02 -8.12173e-01\n",
      "  2.97250e-02  8.44700e-03  2.14885e-01  1.92571e-01 -2.50996e-01\n",
      "  1.68115e-01 -1.80000e-05  5.70470e-02  2.05556e-01  4.13103e-01\n",
      "  2.06672e-01  8.32590e-02 -1.39000e-04 -2.96792e-01  1.35057e-01\n",
      " -4.22060e-02  1.12867e-01 -3.19418e-01 -2.27787e-01 -1.00924e-01\n",
      " -5.06992e-01 -1.29021e-01  1.69623e-01  7.83310e-02  1.43462e-01\n",
      " -4.61020e-01  5.96900e-02 -9.10500e-02 -5.10730e-02  2.95350e-01\n",
      " -3.38894e-01  3.08007e-01 -3.59343e-01 -3.14806e-01  3.42530e-02]\n",
      "77 im [ 0.020528  0.323263  0.305922  0.09337   1.013767 -0.010612  0.08056\n",
      "  0.015969  0.249996 -0.305486  0.021082 -0.086992 -0.127937 -0.105549\n",
      " -0.140462 -0.134706  0.080609  0.063863 -0.02192  -0.086072 -0.368359\n",
      "  0.534333 -0.180565 -0.007581  0.38151  -0.033229 -0.217965  0.128289\n",
      "  0.020212  0.245312 -0.022762  0.45799  -0.058212  0.06779   0.127652\n",
      "  0.254738 -0.058477  0.014509  0.085981 -0.23278   0.199433 -0.07335\n",
      " -0.164618  0.039258 -0.186756 -0.09197   0.058507 -0.09202  -0.438303\n",
      " -0.06211  -0.275159 -0.235951 -0.10363  -0.135933  0.169637 -0.488409\n",
      "  0.060323  0.211227 -0.24929   0.01691   0.300041  0.162083  0.082664\n",
      " -0.11425  -0.67859   0.450564 -0.249952  0.289669  0.433228 -0.275228\n",
      " -0.518024 -0.108288  0.311559 -0.046245  0.692734  0.267306 -0.079967\n",
      "  0.177839 -0.117177 -0.006196  0.166867  0.356881  0.165117 -0.418711\n",
      "  0.429684 -0.19488   0.006099  0.182851  0.307768  0.057891 -0.266012\n",
      "  0.13743  -0.197751 -0.069773 -0.037414 -0.216429 -0.222086 -0.501086\n",
      "  0.028005  0.162911]\n",
      "131 od [-0.013322  0.65003  -0.265014  0.137211  0.80676   0.257879 -0.038284\n",
      " -0.166763  0.206522 -0.457971 -0.412209  0.041403 -0.017622 -0.070818\n",
      "  0.280094 -0.112159  0.478986  0.348481 -0.092296 -0.360081 -0.160839\n",
      "  0.132099 -0.364887 -0.044986  0.250676 -0.303886  0.196478  0.150353\n",
      " -0.001927 -0.174541 -0.136074 -0.100481  0.095591 -0.196138  0.007643\n",
      " -0.002891  0.154189 -0.180003  0.097415  0.057594 -0.070301 -0.029633\n",
      "  0.359585  0.364763 -0.130511  0.008864 -0.08817  -0.259185 -0.961689\n",
      "  0.102438 -0.3342   -0.058859  0.042077  0.064283  0.390441 -0.392776\n",
      " -0.236586  0.267319 -0.032576  0.003358 -0.052075 -0.25431  -0.256393\n",
      "  0.143189 -0.572808  0.435062 -0.362872 -0.123746  0.129472  0.263888\n",
      "  0.521819 -0.003382  0.053653  0.005731  0.35226   0.490936 -0.281009\n",
      "  0.271852 -0.126495  0.385544  0.083981 -0.186466 -0.261689 -0.417042\n",
      "  0.16969  -0.319531 -0.096488 -0.002073  0.097806  0.57094   0.022663\n",
      " -0.052493 -0.181532 -0.215646  0.005666 -0.004128 -0.156959 -0.887606\n",
      " -0.20753  -0.182035]\n",
      "30 al [ 0.111557  0.407735  0.241881 -0.007853  0.431746 -0.214499  0.090969\n",
      "  0.079835  0.052183 -0.128315  0.221626 -0.136121 -0.06478   0.222532\n",
      "  0.0326    0.104974  0.257988  0.228215  0.002728 -0.310492 -0.271473\n",
      "  0.445234  0.031093 -0.021014  0.323984  0.07689   0.03743  -0.095864\n",
      "  0.377894 -0.120821 -0.208202  0.33128  -0.248733 -0.059034  0.122423\n",
      "  0.061962  0.203001 -0.217838  0.14812  -0.16087   0.054699 -0.310452\n",
      " -0.055541  0.184715 -0.330776  0.056889  0.08468  -0.256165 -0.544953\n",
      "  0.268706 -0.080819  0.164951 -0.249177 -0.37173   0.131083 -0.306068\n",
      " -0.124271  0.651151 -0.400077 -0.384145 -0.129976 -0.128335  0.323171\n",
      " -0.202649  0.124747  0.200533 -0.061261  0.366075  0.073863 -0.125289\n",
      " -0.148159 -0.149736  0.090019  0.024099  0.532857  0.186056 -0.469906\n",
      "  0.072895 -0.318152  0.213467  0.340946  0.012982  0.241572 -0.023329\n",
      "  0.312989 -0.221886 -0.250822  0.12547   0.254121  0.018995 -0.008021\n",
      " -0.096859 -0.365448 -0.331441  0.110116 -0.15294  -0.378895  0.069381\n",
      " -0.212661 -0.080517]\n"
     ]
    }
   ],
   "source": [
    "for idx, subw, emb in zip(bpemb_en.encode_ids(\"bimodal\"), bpemb_en.encode(\"bimodal\"),  bpemb_en.embed(\"bimodal\")):\n",
    "    print(idx, subw, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
